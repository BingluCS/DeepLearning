{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/connection.py\", line 174, in _new_conn\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/util/connection.py\", line 95, in create_connection\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/util/connection.py\", line 85, in create_connection\n",
      "OSError: [Errno 101] Network is unreachable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 714, in urlopen\n",
      "    CertificateError,\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 403, in _make_request\n",
      "    # timeouts, check for a zero timeout before making the request.\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 1053, in _validate_conn\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/connection.py\", line 363, in connect\n",
      "    sock=conn,\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/connection.py\", line 186, in _new_conn\n",
      "    def connect(self):\n",
      "urllib3.exceptions.NewConnectionError: <urllib3.connection.HTTPSConnection object at 0x7f4f5f66f7f0>: Failed to establish a new connection: [Errno 101] Network is unreachable\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/requests/adapters.py\", line 486, in send\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/connectionpool.py\", line 798, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/urllib3/util/retry.py\", line 592, in increment\n",
      "urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/bert-base-uncased/tree/main?recursive=True&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4f5f66f7f0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3505, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_1934716/168218939.py\", line 4, in <module>\n",
      "    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py\", line 487, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py\", line 580, in from_pretrained\n",
      "    config_dict, _ = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 550, in get_config_dict\n",
      "    configuration_file = get_configuration_file(\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/configuration_utils.py\", line 841, in get_configuration_file\n",
      "    all_files = get_list_of_files(\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/transformers/file_utils.py\", line 1952, in get_list_of_files\n",
      "    return list_repo_files(path_or_repo, revision=revision, token=token)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_deprecation.py\", line 101, in inner_f\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/hf_api.py\", line 2213, in list_repo_files\n",
      "    return [\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/hf_api.py\", line 2213, in <listcomp>\n",
      "    return [\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/hf_api.py\", line 2179, in list_files_info\n",
      "    for subpath_info in paginate(path=tree_url, headers=headers, params={\"recursive\": True, \"expand\": expand}):\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_pagination.py\", line 35, in paginate\n",
      "    r = session.get(path, params=params, headers=headers)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/requests/sessions.py\", line 602, in get\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/requests/sessions.py\", line 589, in request\n",
      "    return self.request('PUT', url, data=data, **kwargs)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/requests/sessions.py\", line 703, in send\n",
      "    proxies.setdefault(k, v)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/huggingface_hub/utils/_http.py\", line 63, in send\n",
      "    return super().send(request, *args, **kwargs)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/requests/adapters.py\", line 519, in send\n",
      "    raise ConnectionError(e, request=request)\n",
      "requests.exceptions.ConnectionError: (MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /api/models/bert-base-uncased/tree/main?recursive=True&expand=False (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x7f4f5f66f7f0>: Failed to establish a new connection: [Errno 101] Network is unreachable'))\"), '(Request ID: 3c50a08f-38e6-4ea9-9c56-98f44f068ef5)')\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 2102, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1310, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1199, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 1052, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 978, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 878, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/ultratb.py\", line 712, in lines\n",
      "    return self._sd.lines\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "  File \"/home/ubutnu/anaconda3/envs/py38/lib/python3.8/site-packages/executing/executing.py\", line 190, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /home/ubutnu/.cache/huggingface/modules/datasets_modules/datasets/oscar/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2 (last modified on Thu Sep 14 16:34:14 2023) since it couldn't be found locally at oscar., or remotely on the Hugging Face Hub.\n",
      "Found cached dataset oscar (/home/ubutnu/hardDisk/DeepLearning/bert/oscar/unshuffled_deduplicated_it/1.0.0/84838bd49d2295f62008383b05620571535451d84545037bb94d6f3501651df2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e70dc2c48a44dde8209a12185912d3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text'],\n",
       "    num_rows: 28522082\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('oscar', 'unshuffled_deduplicated_it',cache_dir='~/hardDisk/DeepLearning/bert',)\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26fb11dbf59647e694c252dbd31ceda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28522082 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data/text/oscar_it/text_2852.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubutnu/Application/Neural-network/test1.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617075313436227d/home/ubutnu/Application/Neural-network/test1.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m         file_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617075313436227d/home/ubutnu/Application/Neural-network/test1.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617075313436227d/home/ubutnu/Application/Neural-network/test1.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m../../data/text/oscar_it/text_\u001b[39;49m\u001b[39m{\u001b[39;49;00mfile_count\u001b[39m}\u001b[39;49;00m\u001b[39m.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fp: \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a2244617075313436227d/home/ubutnu/Application/Neural-network/test1.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     fp\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(text_data))\n",
      "File \u001b[0;32m~/anaconda3/envs/py38/lib/python3.8/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data/text/oscar_it/text_2852.txt'"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm \n",
    " \n",
    "text_data = [] \n",
    "file_count = 0 \n",
    " \n",
    "for sample in tqdm(dataset['train']): \n",
    "    sample = sample['text'].replace('\\n', '') \n",
    "    text_data.append(sample) \n",
    "    if len(text_data) == 10_000: \n",
    "        # once we git the 10K mark, save to file \n",
    "        with open(f'/home/ubutnu/hardDisk/DeepLearning/data/text/oscar_it/text_{file_count}.txt', 'w', encoding='utf-8') as fp: \n",
    "            fp.write('\\n'.join(text_data)) \n",
    "        text_data = [] \n",
    "        file_count += 1 \n",
    "# after saving in 10K chunks, we will have ~2082 leftover samples, we save those now too \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiHeadAttention(\n",
       "  (attention): DotProductAttention(\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (W_q): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_k): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_v): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (W_o): Linear(in_features=100, out_features=100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "paths = [str(x) for x in Path('../../data/text/oscar_it').glob('**/*.txt')] \n",
    "from tokenizers import ByteLevelBPETokenizer \n",
    "tokenizer = ByteLevelBPETokenizer() \n",
    "tokenizer.train(files=paths[:5], vocab_size=30_522, min_frequency=2, \n",
    "                special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2]) tensor([3, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 100])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, num_queries = 2, 4\n",
    "num_kvpairs, valid_lens =  6, torch.tensor([3, 2])\n",
    "X = torch.ones((batch_size, num_queries, num_hiddens))\n",
    "Y = torch.ones((batch_size, num_kvpairs, num_hiddens))\n",
    "attention(X, Y, Y, valid_lens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device):  #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    for y in prefix[1:]:  # 预热期\n",
    "        _, state = net(get_input(), state)\n",
    "        outputs.append(vocab[y])\n",
    "    for _ in range(num_preds):  # 预测num_preds步\n",
    "        y, state = net(get_input(), state)\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta):  #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm\n",
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"Train a net within one epoch (defined in Chapter 8).\n",
    "\n",
    "    Defined in :numref:`sec_rnn_scratch`\"\"\"\n",
    "    state, timer = None, d2l.Timer()\n",
    "    metric = d2l.Accumulator(2)  # Sum of training loss, no. of tokens\n",
    "    for X, Y in train_iter:\n",
    "        if state is None or use_random_iter:\n",
    "            # Initialize `state` when either it is the first iteration or\n",
    "            # using random sampling\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                # `state` is a tensor for `nn.GRU`\n",
    "                state.detach_()\n",
    "            else:\n",
    "                # `state` is a tuple of tensors for `nn.LSTM` and\n",
    "                # for our custom scratch implementation\n",
    "                for s in state:\n",
    "                    s.detach_()\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        y_hat, state = net(X, state)\n",
    "        print(y_hat.size())\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)\n",
    "            # Since the `mean` function has been invoked\n",
    "            updater(batch_size=1)\n",
    "        metric.add(l * d2l.size(y), d2l.size(y))\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()\n",
    "#@save\n",
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device,\n",
    "              use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity',\n",
    "                            legend=['train'], xlim=[10, num_epochs])\n",
    "    # 初始化\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(\n",
    "            net, train_iter, loss, updater, device, use_random_iter)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))\n",
    "            animator.add(epoch + 1, [ppl])\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 3]), [[1, 2, 3], [2, 3, 4]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[[[1,2,3],[2,3,4]],[[1,2,3],[2,3,4]],[[1,2,3],[2,3,4]],[[1,2,3],[2,3,4]]]\n",
    "dec_attention_weights_2d = [head for blk in x  for head in blk]\n",
    "torch.tensor(x).shape,dec_attention_weights_2d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"in 1940 , the barker 's live @-@ in maid retired , and dorothy barker closed her school at the back of the house in the waldrons\", 'she continued to supervise the household , and to give both her mother and sister the care they needed', \"dorothy and her sister collaborated upon only two books : our darling 's first book and the christian @-@ themed , he leadeth me\", 'in 1954 dorothy barker died of a heart attack', \"barker was unable to pursue her art to any significant extent following her sister 's death , as all the care of her aged mother devolved upon her , but she did manage to begin planning a stained glass window design in her sister 's memory for st. edmund 's , pitlake .\"]\n",
      "['in latin america , an exclusive version of god of war saga , titled god of war : <unk> collection , was released in november 2012', 'the <unk> collection features three blu @-@ ray discs , as opposed to two , with chains of olympus and ghost of sparta included on the third disc', 'it also includes a <unk> game case with exclusive artwork and a limited edition bronze statue of kratos , created by an argentine artist .']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import os\n",
    "def _read_wiki(data_dir):\n",
    "    file_name = os.path.join(data_dir, 'wiki.train.tokens')\n",
    "    with open(file_name, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # 大写字母转换为小写字母\n",
    "    paragraphs = [line.strip().lower().split(' . ')\n",
    "                  for line in lines if len(line.split(' . ')) >= 2]\n",
    "    random.shuffle(paragraphs)\n",
    "    return paragraphs\n",
    "\n",
    "data_dir = d2l.download_extract('wikitext-2', 'wikitext-2')\n",
    "paragraphs = _read_wiki(data_dir)\n",
    "paragraphs = [d2l.tokenize(\n",
    "    paragraph, token='word') for paragraph in paragraphs]\n",
    "# sentences = [sentence for paragraph in paragraphs\n",
    "#                 for sentence in paragraph]\n",
    "for i in range(2):\n",
    "    print(paragraphs[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
