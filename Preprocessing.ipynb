{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"http_proxy\"] = \"http://192.168.104.50:7890\"\n",
    "os.environ[\"https_proxy\"] = \"http://192.168.104.50:7890\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "import torch\n",
    "import random\n",
    "import linecache\n",
    "import re\n",
    "from d2l import torch as d2l\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from datasets import list_datasets, load_dataset\n",
    "from pprint import pprint\n",
    "import h5py\n",
    "import numpy as np\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "# outputs = model(**inputs)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "##model = AutoModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset('sradc/chunked-wikipedia20220301en-bookcorpusopen',cache_dir='~/nvmessd/DeepLearning/datasets/',split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# file_count=0\n",
    "# text_data = []\n",
    "# #save the original data\n",
    "# for sample in tqdm(dataset['text']):\n",
    "#     sample =re.sub(r'\\s+',' ',sample.replace('\\n', ' ')).strip()\n",
    "#     text_data.append(sample)\n",
    "#     if len(text_data) == 10000: \n",
    "#         with open(f'/home/ubutnu/nvmessd/DeepLearning/wiki_book/original/text_{file_count}.txt', 'w', encoding='utf-8') as fp: \n",
    "#             fp.write('\\n'.join(text_data))\n",
    "#         text_data=[]\n",
    "#         file_count += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_mask(tokens,max_len):\n",
    "    tokens = tokens.split(' ')\n",
    "        # 为遮蔽语言模型的输入创建新的词元副本，其中输入可能包含替换的“<mask>”或随机词元\n",
    "    mlm_input_tokens = [token for token in tokens]\n",
    "    pred_positions_and_labels = []\n",
    "    candidate_pred_positions = [i for i in range(len(tokens))]\n",
    "    num_mlm_preds = max(1, round(max_len* 0.15))\n",
    "    # 打乱后用于在遮蔽语言模型任务中获取15%的随机词元进行预测\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80%的时间：将词替换为“<mask>”词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = '[MASK]'\n",
    "        else:\n",
    "            # 10%的时间：保持词不变\n",
    "           #if random.random() < 0.5:\n",
    "            masked_token = tokens[mlm_pred_position]\n",
    "            # 10%的时间：用随机词替换该词\n",
    "            # else:\n",
    "            #     masked_token = random.choice(vocab.idx_to_token)\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        pred_positions_and_labels.append(\n",
    "            (mlm_pred_position, tokens[mlm_pred_position]))\n",
    "    return mlm_input_tokens, pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@save\n",
    "def get_mlm_data_from_tokens(tokens,max_len):\n",
    "    # candidate_pred_positions = []\n",
    "    # # tokens是一个字符串列表\n",
    "    # for i, token in enumerate(tokens):\n",
    "    #     # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "    #     if token in ['<cls>', '<sep>']:\n",
    "    #         continue\n",
    "    #     candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n",
    "    mlm_input_tokens, pred_positions_and_labels = replace_mask(tokens,max_len)\n",
    "    pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
    "                                       key=lambda x: x[0])\n",
    "    pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "    mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "    return \" \".join(mlm_input_tokens), pred_positions,\" \".join(mlm_pred_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30522"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Vocab:\n",
    "    def __init__(self,file):\n",
    "        with open(file, 'r') as file:\n",
    "            content = file.read()\n",
    "        self.idx_to_token = content.split()\n",
    "        self.token_to_idx = {token: idx\n",
    "            for idx, token in enumerate(self.idx_to_token)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "    \n",
    "    def __getitem__(self,tokens):\n",
    "        if not isinstance(tokens, (list, tuple)):\n",
    "            return self.token_to_idx.get(tokens, self.unk)\n",
    "        return [self.__getitem__(token) for token in tokens]\n",
    "    \n",
    "    def to_tokens(self, indices):\n",
    "        if not isinstance(indices, (list, tuple)):\n",
    "            return self.idx_to_token[indices]\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "    \n",
    "    def unk(self):  # 未知词元的索引为0\n",
    "        return 100\n",
    "# 使用空格分割字符串，得到单词列表\n",
    "vocab=Vocab('./vocab.txt')\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_num = [i for i in range(1, len(vocab)) if i not in (101, 102)]\n",
    "# type(random.choice(rand_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_mask_and_label(input_ids):\n",
    "    len_tokens=len(list(filter(lambda x: x != 0, input_ids)))\n",
    "\n",
    "    num_mlm_preds = max(1, round(len_tokens* 0.15)-2)\n",
    "    candidate_pred_positions=list(range(1,len_tokens-1))\n",
    "    random.shuffle(candidate_pred_positions)\n",
    "\n",
    "    pred_positions_and_labels=[]\n",
    "    mlm_input_tokens=input_ids\n",
    "    for mlm_pred_position in candidate_pred_positions:\n",
    "        if len(pred_positions_and_labels) >= num_mlm_preds:\n",
    "            break\n",
    "        masked_token = None\n",
    "        # 80%的时间：将词替换为“<mask>”词元\n",
    "        if random.random() < 0.8:\n",
    "            masked_token = 103\n",
    "        else:\n",
    "            # 10%的时间：保持词不变\n",
    "            if random.random() < 0.5:\n",
    "                masked_token = input_ids[mlm_pred_position]\n",
    "            # 10%的时间：用随机词替换该词\n",
    "            else:\n",
    "                masked_token = random.choice(rand_num)\n",
    "        pred_positions_and_labels.append(\n",
    "            (mlm_pred_position,input_ids[mlm_pred_position]))\n",
    "        mlm_input_tokens[mlm_pred_position] = masked_token\n",
    "        #print(pred_positions_and_labels)\n",
    "    return mlm_input_tokens,pred_positions_and_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "701e52ee604941e68bfe5a5c7b0b4b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# file_count=0\n",
    "# text_data,pred,labels = [],[],[]\n",
    "# max_len=512\n",
    "# i=0\n",
    "# path='/home/ubutnu/nvmessd/DeepLearning/wiki_book'\n",
    "# #front = [x.replace('\\n', '').split(' ') for x in paragraphs[:10]]\n",
    "# # front = paragraphs[0].replace('\\n', '')\n",
    "# mlm_input_tokens_all,pred_positions_all,mlm_pred_labels_all,token_type_ids_all,attention_mask_all = [],[],[],[],[]\n",
    "# for file_idx in tqdm(range(3353)):\n",
    "#     text_path = path+'/original/text_%d.txt'%(file_idx)\n",
    "#     for content_idx in range(1,10001):\n",
    "#         text = linecache.getline(text_path,content_idx).strip()\n",
    "\n",
    "#         inputs = tokenizer(text,truncation=True,padding='max_length',max_length=max_len)\n",
    "#         input_ids,token_type_ids,attention_mask=inputs['input_ids'],inputs['token_type_ids'],inputs['attention_mask']\n",
    "\n",
    "#         mlm_input_tokens, pred_positions_and_labels  = add_mask_and_label(input_ids)\n",
    "        \n",
    "#         pred_positions_and_labels = sorted(pred_positions_and_labels,\n",
    "#                                         key=lambda x: x[0])\n",
    "#         pred_positions = [v[0] for v in pred_positions_and_labels]\n",
    "#         mlm_pred_labels = [v[1] for v in pred_positions_and_labels]\n",
    "#         pred_positions.extend([0]*(512-len(pred_positions)))\n",
    "#         mlm_pred_labels.extend([0]*(512-len(mlm_pred_labels)))\n",
    "\n",
    "#         pred_positions_all.extend(pred_positions)\n",
    "#         mlm_pred_labels_all.extend(mlm_pred_labels)\n",
    "\n",
    "#         attention_mask_all.extend(attention_mask)\n",
    "#         token_type_ids_all.extend(token_type_ids)\n",
    "#         mlm_input_tokens_all.extend(mlm_input_tokens)\n",
    "#     if (file_idx+1)%10==0:\n",
    "#         mlm_input_path = path+'/inputm/inputm_%d.h5'%(i)\n",
    "#         pred_path = path+'/pred/pred_%d.h5'%(i)\n",
    "#         label_path = path+'/label/label_%d.h5'%(i)\n",
    "#         attention_mask_path = path+'/attm/attm_%d.h5'%(i)\n",
    "#         type_path = path+'/type/type_%d.h5'%(i)\n",
    "#         with h5py.File(mlm_input_path, 'w') as h5f:\n",
    "#             h5f.create_dataset('data', data=mlm_input_tokens_all) \n",
    "#         with h5py.File(type_path, 'w') as h5f:\n",
    "#             h5f.create_dataset('data', data=token_type_ids_all) \n",
    "#         with h5py.File(attention_mask_path, 'w') as h5f:\n",
    "#             h5f.create_dataset('data', data=attention_mask_all) \n",
    "#         with h5py.File(pred_path, 'w') as h5f:\n",
    "#             h5f.create_dataset('data', data=pred_positions_all) \n",
    "#         with h5py.File(label_path, 'w') as h5f:\n",
    "#             h5f.create_dataset('data', data=mlm_pred_labels_all) \n",
    "#         i+=1\n",
    "#         mlm_input_tokens_all,pred_positions_all,mlm_pred_labels_all,token_type_ids_all,attention_mask_all = [],[],[],[],[]\n",
    "\n",
    "# mlm_input_path = path+'/inputm/inputm_%d.h5'%(i)\n",
    "# pred_path = path+'/pred/pred_%d.h5'%(i)\n",
    "# label_path = path+'/label/label_%d.h5'%(i)\n",
    "# attention_mask_path = path+'/attm/attm_%d.h5'%(i)\n",
    "# type_path = path+'/type/type_%d.h5'%(i)\n",
    "# with h5py.File(mlm_input_path, 'w') as h5f:\n",
    "#     h5f.create_dataset('data', data=mlm_input_tokens_all) \n",
    "# with h5py.File(type_path, 'w') as h5f:\n",
    "#     h5f.create_dataset('data', data=token_type_ids_all) \n",
    "# with h5py.File(attention_mask_path, 'w') as h5f:\n",
    "#     h5f.create_dataset('data', data=attention_mask_all) \n",
    "# with h5py.File(pred_path, 'w') as h5f:\n",
    "#     h5f.create_dataset('data', data=pred_positions_all) \n",
    "# with h5py.File(label_path, 'w') as h5f:\n",
    "#     h5f.create_dataset('data', data=mlm_pred_labels_all) \n",
    "# mlm_input_tokens_all,pred_positions_all,m\n",
    "\n",
    "\n",
    "# lm_pred_labels_all,token_type_ids_all,attention_mask_all = [],[],[],[],[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3cfb675f9894477a59808c2a5dc6a76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import h5py\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "onepath='/home/ubutnu/nvmessd/DeepLearning/wiki_bookold'\n",
    "twopath='/home/ubutnu/nvmessd/DeepLearning/wiki_book'\n",
    "zippath='/home/ubutnu/nvmessd/DeepLearning/wiki_bookzip'\n",
    "\n",
    "def one_two_zip(path,num):\n",
    "    with h5py.File(onepath+path, 'r') as f:\n",
    "        data=f['data'][:]\n",
    "        if len(data) % num != 0:\n",
    "            raise ValueError(\"数据长度不能被整除\")\n",
    "    data1=data.reshape(-1,num)\n",
    "    with h5py.File(twopath+path, 'w') as new_file:\n",
    "        new_file.create_dataset('data', data=data1)\n",
    "    with h5py.File(zippath+path, 'w') as new_file:\n",
    "        new_file.create_dataset('data', data=data1,compression='gzip',compression_opts=9)\n",
    "\n",
    "def one_two_zipe(path,num):\n",
    "    with h5py.File(onepath+path, 'r') as f:\n",
    "        data=f['data'][:]\n",
    "        # if len(data) % num != 0:\n",
    "        #     raise ValueError(\"数据长度不能被整除\")\n",
    "    data1=data.reshape(-1,num)[:,:70]\n",
    "    \n",
    "    # if data1.shape[0] % 70 != 0:\n",
    "    #     raise ValueError(\"数据长度不能被整除\")\n",
    "    with h5py.File(twopath+path, 'w') as new_file:\n",
    "        new_file.create_dataset('data', data=data1)\n",
    "    with h5py.File(zippath+path, 'w') as new_file:\n",
    "        new_file.create_dataset('data', data=data1,compression='gzip',compression_opts=9)\n",
    "\n",
    "def ctoc(path):\n",
    "    with h5py.File(twopath+path, 'r') as f:\n",
    "        data=f['data'][:]\n",
    "    with h5py.File(zippath+path, 'w') as new_file:\n",
    "        new_file.create_dataset('data', data=data,compression='gzip',compression_opts=9)\n",
    "\n",
    "for file_idx in tqdm(range(336)):\n",
    "    inputm_path = '/inputm/inputm_%d.h5'%(file_idx)\n",
    "    attm_path = '/attm/attm_%d.h5'%(file_idx)\n",
    "    pred_path = '/pred/pred_%d.h5'%(file_idx)\n",
    "    labels_path = '/label/label_%d.h5'%(file_idx)\n",
    "    weight_path = '/weight/weight_%d.h5'%(file_idx)\n",
    "    ctoc(inputm_path)\n",
    "    ctoc(attm_path)\n",
    "    ctoc(pred_path)\n",
    "    ctoc(labels_path)\n",
    "    ctoc(weight_path)\n",
    "    # one_two_zip(inputm_path,512)\n",
    "    # one_two_zip(attm_path,512)\n",
    "    # one_two_zipe(pred_path,512)\n",
    "    # one_two_zipe(labels_path,512)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41cf0853ea6444dbb651425df9d32972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/336 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# for file_idx in tqdm(range(336)):\n",
    "#     pred_path = '/pred/pred_%d.h5'%(file_idx)\n",
    "#     out = '/weight/weight_%d.h5'%(file_idx)\n",
    "#     with h5py.File(twopath+pred_path, 'r') as f:\n",
    "#         data=f['data'][:]\n",
    "#     # non_zero_counts = np.count_nonzero(data, axis=1)\n",
    "#     # weight=np.zeros_like(data)\n",
    "#     # for i, count in enumerate(non_zero_counts):\n",
    "#     #     weight[i, :count] = 1\n",
    "\n",
    "#     # with h5py.File(twopath+out, 'w') as new_file:\n",
    "#     #     new_file.create_dataset('data', data=weight)\n",
    "#     with h5py.File(zippath+out, 'w') as new_file:\n",
    "#         new_file.create_dataset('data', data=weight,compression='gzip',compression_opts=9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text,mask = [],[]\n",
    "# for i in tqdm(range(count)):\n",
    "#     with open('/home/ubutnu/nvmessd/DeepLearning/wiki_book_id/text_%d.txt'%(i), 'r', encoding='utf-8') as fp: \n",
    "#         lines = fp.read().split('\\n')\n",
    "#         text.append(lines)\n",
    "#     with open('/home/ubutnu/nvmessd/DeepLearning/wiki_book_mask/text_%d.txt'%(i), 'r', encoding='utf-8') as fp:\n",
    "#         lines = fp.read().split('\\n')\n",
    "#         mask.append(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=tokenizer.batch_encode_plus(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Wiki_book_Dataset(torch.utils.data.Dataset): \n",
    "    def __init__(self, train_path,max_len): \n",
    "        self.train_path=train_path\n",
    "        self.max_len=max_len\n",
    "        self.max_num_mlm_preds = round(max_len * 0.15)\n",
    "        #self.rand_MASk=rand_MASk()\n",
    "        # if rand_MASk is None:\n",
    "        #     self.text=text\n",
    "        # else:\n",
    "        #     self.text=rand_MASk(text)\n",
    "\n",
    "    \n",
    "    def __len__(self): \n",
    "        # return the number of samples \n",
    "        return 33530000\n",
    " \n",
    "    def __getitem__(self, i):\n",
    "        file_idx,content_idx = i/10000,i%10000+1\n",
    "        path_id = self.train_path+'/wiki_book_id/text_%d.txt'%(file_idx)\n",
    "        path_mask = self.train_path+'/wiki_book_mask/text_%d.txt'%(file_idx)\n",
    "        #path_pred = self.train_path+'/wiki_book_pred/text_pred_%d.txt'%(file_idx)\n",
    "\n",
    "        #path_label=self.train_path+'/wiki_book_label/text_%d.txt'%(file_idx)\n",
    "        id = linecache.getline(path_id,content_idx).replace('\\n', '').split(' ')\n",
    "        mask = linecache.getline(path_mask,content_idx).replace('\\n', '').split(' ')\n",
    "\n",
    "        all_pred_positions, all_labels= [],[]\n",
    "        #pred_positions = linecache.getline(path_pred,content_idx).replace('\\n', '').split(' ').remove(' ')\n",
    "        # labels = linecache.getline(path_pred,content_idx).replace('\\n', '').split(' ')\n",
    "\n",
    "        id_tensor=torch.tensor(list(map(int,id)))\n",
    "        mask_tensor=torch.tensor(list(map(int,mask)))\n",
    "        #pred_tensor=[int(float(pred)) for pred in pred_positions]\n",
    "        #labels_tensor=torch.tensor(list(map(int,labels)))\n",
    "        \n",
    "        # all_pred_positions.append(torch.tensor(pred_tensor + [0] * (\n",
    "        #     self.max_num_mlm_preds - len(pred_positions)), dtype=torch.long))\n",
    "        # all_labels.append(torch.tensor(labels_tensor + [0] * (\n",
    "        #     self.max_num_mlm_preds - len(labels)), dtype=torch.long))\n",
    "        #mask\n",
    "\n",
    "\n",
    "\n",
    "        # if i <=2:\n",
    "        #     print(ls)\n",
    "        return id_tensor,mask_tensor#,pred_tensor#,labels_tensor\n",
    "    #torch.tensor(list(map(int,text)))\n",
    "    # candidate_pred_positions = []\n",
    "    # # tokens是一个字符串列表\n",
    "    # for i, token in enumerate(tokens):\n",
    "    #     # 在遮蔽语言模型任务中不会预测特殊词元\n",
    "    #     if token in ['<cls>', '<sep>']:\n",
    "    #         continue\n",
    "    #     candidate_pred_positions.append(i)\n",
    "    # 遮蔽语言模型任务中预测15%的随机词元\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len=512\n",
    "train_path='/home/ubutnu/nvmessd/DeepLearning'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset=Wiki_book_Dataset(train_path,max_len)\n",
    "# train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32,num_workers=8,shuffle=False)\n",
    "# for i,(X,y,pre) in enumerate(tqdm(train_loader)):\n",
    "#     #ls=X[1].split(' ')\n",
    "#     #ls=list(map(int,X[1]))\n",
    "#     print(X.size(),i)\n",
    "\n",
    "    #X, y = X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(BERTEncoder, self).__init__()\n",
    "        self.bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "def train(model, train_path, learning_rate, epochs,max_len):\n",
    "  # 通过Dataset类获取训练和验证集\n",
    "    #train, val = Dataset(train_data), Dataset(val_data)\n",
    "    # DataLoader根据batch_size获取数据，训练时选择打乱样本\n",
    "    train_dataset=Wiki_book_Dataset(train_path,max_len)\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=8,num_workers=8,shuffle=True)\n",
    "  # 判断是否使用GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    if use_cuda:\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "    # 开始进入训练循环\n",
    "    for epoch_num in range(epochs):\n",
    "      # 定义两个变量，用于存储训练集的准确率和损失\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "      # 进度条函数tqdm\n",
    "            for input_id,mask in tqdm(train_loader):\n",
    "                \n",
    "                #train_label = train_label.to(device)\n",
    "                mask = mask.to(device)\n",
    "                input_id = input_id.to(device)\n",
    "        # 通过模型得到输出\n",
    "                output = model(input_id, attention_mask=mask)\n",
    "                # 计算损失\n",
    "        #         batch_loss = criterion(output, train_label)\n",
    "        #         total_loss_train += batch_loss.item()\n",
    "        #         # 计算精度\n",
    "        #         acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "        #         total_acc_train += acc\n",
    "        # # 模型更新\n",
    "        #         model.zero_grad()\n",
    "        #         batch_loss.backward()\n",
    "        #         optimizer.step()\n",
    "            # ------ 验证模型 -----------\n",
    "            # 定义两个变量，用于存储验证集的准确率和损失\n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "      # 不需要计算梯度\n",
    "        #     with torch.no_grad():\n",
    "        #         # 循环获取数据集，并用训练好的模型进行验证\n",
    "        #         for val_input, val_label in val_dataloader:\n",
    "        #   # 如果有GPU，则使用GPU，接下来的操作同训练\n",
    "        #             val_label = val_label.to(device)\n",
    "        #             mask = val_input['attention_mask'].to(device)\n",
    "        #             input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "  \n",
    "        #             output = model(input_id, mask)\n",
    "\n",
    "        #             batch_loss = criterion(output, val_label)\n",
    "        #             total_loss_val += batch_loss.item()\n",
    "                    \n",
    "        #             acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "        #             total_acc_val += acc\n",
    "            \n",
    "            # print(\n",
    "            #     f'''Epochs: {epoch_num + 1} \n",
    "            #   | Train Loss: {total_loss_train / len(train_data): .3f} \n",
    "            #   | Train Accuracy: {total_acc_train / len(train_data): .3f} \n",
    "            #   | Val Loss: {total_loss_val / len(val_data): .3f} \n",
    "            #   | Val Accuracy: {total_acc_val / len(val_data): .3f}''')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0140f9b8f1eb45489b339c6ca18c616a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4191250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train(model,train_path,0.001,5,512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
